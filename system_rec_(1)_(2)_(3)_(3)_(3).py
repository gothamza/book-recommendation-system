# -*- coding: utf-8 -*-
"""SYSTEM_REC_(1)_(2)_(3)_(3)_(3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V9Ldzf2Z8lYDKCoBhwB95DMFmXiFkvuF

## **Import**
"""

# !pip install --upgrade scikit-learn
# !pip install surprise
# !pip install scikit-surprise

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import gzip
import json
import re
import os
from tqdm import tqdm
import requests
from io import BytesIO
from pickle import dump, load
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import pairwise_distances
import scipy.sparse
from scipy.sparse import hstack
from scipy import stats
from scipy.sparse import coo_matrix
from surprise import Reader, Dataset
from surprise import SVD, model_selection, accuracy
from PIL import Image
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.vgg16 import preprocess_input
#from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Model

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

# Example usage:
# plot_confusion_matrix(y_true, y_pred, classes=class_names, normalize=True,
#                       title='Normalized confusion matrix')

"""## ***Parsers***"""

def user_parser(line):
  data = {
      'book_id': line['book_id'],
      'user_id': line['user_id'],
      'review_text': line['review_text'],
      'n_votes': line['n_votes'],
      'user_rating': line['rating']
  }
  return data

def book_parser(line):
  data = {
      'book_id': line['book_id'],
      'title_without_series': line['title_without_series'],
      'book_description': line['description'],
      'publication_year': line['publication_year'],
      'publisher': line['publisher'],
      'ratings_count': line['ratings_count'],
      'book_average_rating': line['average_rating'],
      'cover_page': line['image_url'],
      'book_url': line['url'],
      'is_ebook': line['is_ebook'],
      'num_pages': line['num_pages'],
  }
  return data

"""## **Data books**"""

df_books = pd.read_csv('/content/drive/MyDrive/sys-rec/FatimaEzz/REC_SYS/goodreads/CSV/df_books.csv.gz')

df_books

"""## **Data users**"""

df_users = pd.read_csv('/content/drive/MyDrive/sys-rec/FatimaEzz/REC_SYS/goodreads/CSV/df_user.csv')

df_users

"""## **Preprocessing**"""

stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've",\
            "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \
            'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their',\
            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', \
            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \
            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \
            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\
            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\
            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\
            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \
            's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', \
            've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn',\
            "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn',\
            "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", \
            'won', "won't", 'wouldn', "wouldn't"])

def decontracted(phrase):
    # specific
    phrase = re.sub(r"won't", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)

    # general
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    return phrase

def preprocess_numeric_data(dataframe, df_name):
  if df_name == 'df_books_copy':
    dataframe["book_average_rating"] = pd.to_numeric(dataframe["book_average_rating"])
    dataframe["publication_year"] = pd.to_numeric(dataframe["publication_year"])
    dataframe["ratings_count"] = dataframe["ratings_count"].astype(int)
    dataframe["num_pages"] = pd.to_numeric(dataframe["num_pages"])
    dataframe["is_ebook"] = dataframe["is_ebook"].map({True:1, False:0})
  else:
    dataframe["rating"] = dataframe["rating"].astype(int)
    dataframe["book_id"] = dataframe["book_id"].astype(int)
    dataframe["n_votes"] = dataframe["n_votes"].astype(int)
  return dataframe

def preprocess_text_data(dataframe, df_name):
  dataframe["book_id"] = dataframe["book_id"].astype(str)
  if df_name == 'df_books_numeric_mod':
    dataframe["mod_title"] = dataframe["title_without_series"].astype(str).apply(decontracted)
    dataframe["mod_title"] = dataframe["mod_title"].str.replace("[^a-zA-Z0-9 ]", "", regex=True)
    dataframe["mod_title"] = dataframe["mod_title"].astype(str).apply(lambda sentence: ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords and len(e.lower())>1))
    dataframe["mod_title"] = dataframe["mod_title"].str.replace("\s+", " ", regex=True)
    dataframe = dataframe[dataframe["mod_title"].str.len() > 0]

    dataframe["book_description"] = dataframe["book_description"].str.replace("[^a-zA-Z0-9 ]", "", regex=True)
    dataframe["book_description"] = dataframe["book_description"].str.lower()
    dataframe["book_description"] = dataframe["book_description"].str.replace("\s+", " ", regex=True)
    dataframe = dataframe[dataframe["book_description"].str.len() > 0]

    dataframe["publisher"] = dataframe["publisher"].str.replace("[^a-zA-Z0-9 ]", "", regex=True)
    dataframe["publisher"] = dataframe["publisher"].str.lower()
    dataframe["publisher"] = dataframe["publisher"].str.replace("\s+", " ", regex=True)
    dataframe = dataframe[dataframe["publisher"].str.len() > 0]
  else:
    dataframe["user_id"] = dataframe["user_id"].astype(str)

  return dataframe

df_books_copy = df_books.copy()
df_books_numeric_mod = preprocess_numeric_data(df_books_copy, 'df_books_copy')
df_books_processed = preprocess_text_data(df_books_numeric_mod, 'df_books_numeric_mod')
df_users_numeric_mod = preprocess_numeric_data(df_users, 'df_users')
df_users_processed = preprocess_text_data(df_users_numeric_mod, 'df_users_numeric_mod')

df_books_processed[['mod_title']].duplicated().sum()

df_books_processed[['book_average_rating']].duplicated().sum()

df_books_processed.drop_duplicates(subset=['mod_title', 'book_average_rating'], keep='first', inplace=True)
df_books_processed.shape

df_books_processed[['mod_title', 'book_average_rating']].duplicated().sum()

df_books_processed[['title_without_series']].duplicated().sum()

df_books_processed[['title_without_series']].drop_duplicates()

df_books_processed.drop_duplicates()

df_books_processed.isnull().any(axis=0)

df_books_processed.isnull().sum()

df_books_processed.duplicated().sum()

df_books_processed[df_books_processed.isnull().any(axis=1)][['ratings_count', 'book_average_rating']].describe()

df_books_processed['publication_year'] = df_books_processed['publication_year'].fillna(df_books_processed['publication_year'].median())
df_books_processed['num_pages'] = df_books_processed['num_pages'].fillna(df_books_processed['num_pages'].median())

df_books_processed.isnull().sum()

df_users_processed[['user_id','book_id','rating']].duplicated().sum()

df_users_processed.isnull().any(axis=0)

df_users_processed.dropna(axis=0, inplace=True)

df_users_processed.isnull().any(axis=0)

print('df_users_processed features: ', list(df_users_processed.columns))
print('df_books_processed features: ', list(df_books_processed.columns))

print('df_users_processed shape: ', df_users_processed.shape)
print('df_books_processed shape: ', df_books_processed.shape)

df_books_users = pd.merge(df_books_processed, df_users_processed, on='book_id', how='inner')

df_books_users.isnull().any()

"""## **EDA**

### n_votes
"""

for i in range(0, 110, 10):
  print(f'{i}th percentile is', np.percentile(df_books_users['n_votes'], i))

figure, axes = plt.subplots(1, 2, figsize=(18,8))
sns.distplot(df_books_users['n_votes'], ax=axes[0], color='red')
sns.boxplot(y= 'n_votes', data=df_books_users, ax=axes[1],  x='rating')
plt.show()

df_books_users.drop(columns=['n_votes'], inplace=True)

df_books_users

"""### n_pages"""

for i in range(0, 110, 10):
  print(f'{i}th percentile is', np.percentile(df_books_users['num_pages'], i))

for i in range(0, 11, 1):
  print(f'{i}th percentile is', np.percentile(df_books_users['num_pages'], i))

for i in range(90, 101, 1):
  print(f'{i}th percentile is', np.percentile(df_books_users['num_pages'], i))

print('num_pages >=55000 :   ', df_books_users[df_books_users['num_pages']>=55000.0]['num_pages'].count())
print('num_pages <=4 :       ', df_books_users[(df_books_users['num_pages']<=4)]['num_pages'].count())

df_books_users = df_books_users[df_books_users['num_pages']>4]

figure, axes = plt.subplots(1, 2, figsize=(25,8))
filter_data = df_books_users[df_books_users['num_pages']<=800.0]
sns.distplot(filter_data['num_pages'], ax=axes[0], color='red')
sns.boxplot(y= 'num_pages', data=filter_data[(filter_data['book_average_rating']>2.6) & (filter_data['book_average_rating']<4.8)] , ax=axes[1],  x='rating')
plt.show()

"""###publication_year"""

for i in range(0, 110, 10):
  print(f'{i}th percentile is', np.percentile(df_books_users['publication_year'], i))

for i in range(90, 101, 1):
  print(f'{i}th percentile is', np.percentile(df_books_users['publication_year'], i))

for i in range(0, 11, 1):
  print(f'{i}th percentile is', np.percentile(df_books_users['publication_year'], i))

print('publication_year <1850.0 :-   ', df_books_users[df_books_users['publication_year']<1850.0]['publication_year'].count())
print('publication_year >2022.0 :-   ', df_books_users[df_books_users['publication_year']>2022.0]['publication_year'].count())

df_books_users = df_books_users[(df_books_users['publication_year']>=1850.0)]
df_books_users = df_books_users[(df_books_users['publication_year']<=2022.0)]

figure, axes = plt.subplots(1, 2, figsize=(25,8))
sns.distplot(df_books_users['publication_year'], ax=axes[0], color='red')
sns.boxplot(y= 'publication_year', data=df_books_users[(df_books_users['book_average_rating']>2.6) & (df_books_users['book_average_rating']<4.8)] , ax=axes[1],  x='rating')
plt.show()

"""## book_average_rating"""

for i in range(0, 110, 10):
  print(f'{i}th percentile is', np.percentile(df_books_users['book_average_rating'], i))

figure, axes = plt.subplots(1, 2, figsize=(25,8))
sns.distplot(df_books_users['book_average_rating'], ax=axes[0], color='orange')
sns.boxplot(y= 'book_average_rating', data=df_books_users, ax=axes[1],  x='rating')
plt.show()

"""## is_ebook"""

for i in range(0, 110, 10):
  print(f'{i}th percentile is', np.percentile(df_books_users['is_ebook'], i))

figure, axes = plt.subplots(1, 2, figsize=(25,8))
sns.distplot(df_books_users['is_ebook'], ax=axes[0], color='blue')
sns.boxplot(y= 'is_ebook', data=df_books_users[(df_books_users['book_average_rating']>2.6) & (df_books_users['book_average_rating']<4.8)], ax=axes[1],  x='rating')
plt.show()

"""## ratings_count"""

for i in range(0, 110, 10):
  print(f'{i}th percentile is', np.percentile(df_books_users['ratings_count'], i))

figure, axes = plt.subplots(1, 2, figsize=(25,8))
filter_data = df_books_users[df_books_users['ratings_count']<=18935.0]
sns.distplot(filter_data['ratings_count'], ax=axes[0], color='yellow')
sns.boxplot(y= 'ratings_count', data=filter_data[(filter_data['book_average_rating']>2.6) & (filter_data['book_average_rating']<4.8)] , ax=axes[1],  x='rating')
plt.show()

"""## Distplots and Countplots Analysis/ Conclusion

**Distplot**
"""

filtered_dataset = df_books_processed[(df_books_processed['book_average_rating']>4.0) & (df_books_processed['ratings_count']>8000) & (df_books_processed['publication_year']>1950)]


figure, axes = plt.subplots(2, 2, figsize=(30,10))
sns.distplot(filtered_dataset['book_average_rating'], ax=axes[0,0], label='Average Rating on Books', color='orange')
sns.distplot(filtered_dataset['num_pages'], ax=axes[0,1], label='Number of Pages in Books',color="green")
sns.distplot(filtered_dataset['publication_year'], ax=axes[1,0], label='Publication Year of Books',color="blue")
sns.distplot(filtered_dataset['is_ebook'], ax=axes[1,1], label='E-Book/Paper Book',color="black")
plt.show()

"""**Countplots**"""

plt.style.use('ggplot')
plt.figure(figsize=(12, 5))
sns.countplot(data = df_books_users, x= 'rating', palette='Set2')
plt.show()

plt.style.use('ggplot')
plt.figure(figsize=(32, 8))
sns.countplot(data=filtered_dataset[filtered_dataset['num_pages']<=100] , x='num_pages', palette='husl')
plt.show()

plt.style.use('fivethirtyeight')
plt.figure(figsize=(32, 8))
sns.countplot(data=filtered_dataset[filtered_dataset['publication_year']>2000] , x='publication_year', palette='pastel')
plt.show()

"""# **Correlation Analysis between each numeric feature**"""

cormat = df_books_users[['publication_year', 'ratings_count', 'book_average_rating', 'num_pages', 'rating']].corr()
round(cormat,2)
plt.figure(figsize=(12, 8))
sns.heatmap(cormat, cmap='cividis', annot=True, linewidths=2)
plt.show()

"""### Saving df_books_users_processed dataframe"""

df_books_users_processed = df_books_users
df_books_users_processed.to_csv('df_books_users_processed.csv.gz', index=False, compression='gzip')

df_books_users_processed.isnull().any()

df_books_processed = pd.read_csv('/content/df_books_users_processed.csv.gz')
df_books_users_processed = pd.read_csv('/content/df_books_users_processed.csv.gz')

df_books_users_processed = df_books_users_processed.dropna(axis=0)
df_books_processed = df_books_processed[df_books_processed['mod_title'].notnull()]

"""# **First Cut Solution**

### Top 50 books with best book_average_rating
"""

for i in range(0, 110, 10):
  print(f'{i}th percentile is', np.percentile(df_books_processed['ratings_count'], i))

for i in range(90, 101):
  print(f'{i}th percentile is', np.percentile(df_books['ratings_count'], i))

import pandas as pd

def make_clickable(url):
    return f'<a target="_blank" href="{url}">{url}</a>'

def show_image(url):
    return f'<img src="{url}" width="60" >'
def top_book_average_rating(min_rating=3.50):
  top50_highest_rated_books = df_books_processed[(df_books_processed['book_average_rating']>=min_rating) & (df_books_processed['ratings_count']>8500.0)].sort_values(by='book_average_rating', ascending=False)
  top50_highest_rated_books[:50].to_csv('top50_highest_rated_books.csv', index=False)
  return top50_highest_rated_books[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})
top_book_average_rating(3.00)

"""### Top 50 concised books"""

df_books['num_pages'].describe()

def top_concised_books():
  top_50_concised_books = df_books_processed[(df_books_processed['num_pages']<=300)& (df_books_processed['ratings_count']>3000.0)&(df_books_processed['book_average_rating']>=4.50)].sort_values(by='book_average_rating', ascending=False)
  top_50_concised_books.to_csv('top_50_concised_books.csv', index=False)
  return top_50_concised_books[['book_id', 'title_without_series', 'num_pages', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

top_concised_books()

"""### Books having similar book title"""

vectorizer_title_fc = TfidfVectorizer()
tfidf_title_fc = vectorizer_title_fc.fit_transform(df_books_processed["mod_title"])

with open('/content/vectorizer_title.pkl', 'wb') as p:
    dump(vectorizer_title_fc, p)

with open('/content/tfidf_title.pkl', 'wb') as p:
    dump(tfidf_title_fc, p)

with open("/content/vectorizer_title.pkl", "rb") as p:
    vectorizer_title_fc = load(p)

with open("/content/tfidf_title.pkl", "rb") as p:
    tfidf_title_fc = load(p)

def top_50_similar_title_books(vectorizer):
  query = str(input('Enter book title: '))
  processed = re.sub("[^a-zA-Z0-9 ]", "", query.lower())
  query_vec = vectorizer.transform([query])
  similarity = cosine_similarity(query_vec, tfidf_title_fc).flatten()
  indices = np.argpartition(similarity, -50)[-50:]
  results = df_books_processed.iloc[indices]
  return results[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

top_50_similar_title_books(vectorizer_title_fc)

"""### Books where user choice's match"""

def similar_user_df(df_books_users):
  user_id = str(input("Enter the user_id: "))
  df_liked_books = df_books_users[df_books_users['user_id']==user_id]
  liked_books = set(df_liked_books['book_id'])
  top_5_liked_books = df_liked_books.sort_values(by='rating', ascending=False)['book_id'][:5]
  similar_user = df_books_users[(df_books_users['book_id'].isin(top_5_liked_books))& (df_books_users['rating']>4)]['user_id']
  data = df_books_users[(df_books_users['user_id'].isin(similar_user))][['user_id', 'book_id', 'rating', 'ratings_count','title_without_series', 'book_average_rating', 'book_url', 'cover_page']]
  return data, liked_books

def popular_recommendation(recs, liked_books):
  all_recs = recs["book_id"].value_counts()
  all_recs = all_recs.to_frame().reset_index()
  all_recs.columns = ["book_id", "book_count"]
  all_recs = all_recs.merge(recs, how="inner", on="book_id")
  all_recs["score"] = all_recs["book_count"] * (all_recs["book_count"] / all_recs["ratings_count"])
  popular_recs = all_recs.sort_values("score", ascending=False)
  popular_recs_unbiased = popular_recs[~popular_recs["book_id"].isin(liked_books)].drop_duplicates(subset=['title_without_series'])
  popular_recs_unbiased.to_csv('popular_recs.csv', index=False)
  return popular_recs_unbiased[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page', 'ratings_count']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

recs, liked_books = similar_user_df(df_books_users)
popular_recommendation(recs, liked_books)

"""### Top 50 e-book"""

def top_50_paper_books():
  df_ebook = df_books_processed[(df_books_processed['book_average_rating']>=4.5) & (df_books_processed['ratings_count']>3000.0)].sort_values(by='book_average_rating', ascending=False)
  top_50_paper_books = df_ebook[df_ebook['is_ebook']==False]
  top_50_paper_books.to_csv('top_50_paper_books.csv', index=False)
  return top_50_paper_books[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

top_50_paper_books()

"""### Books having similar description"""

vectorizer_description = TfidfVectorizer()
tfidf_description = vectorizer_description.fit_transform(df_books_processed["book_description"])

with open('/content/vectorizer_description.pkl', 'wb') as p:
    dump(vectorizer_description, p)

with open('/content/tfidf_description.pkl', 'wb') as p:
    dump(tfidf_description, p)

with open("/content/vectorizer_description.pkl", "rb") as p:
    vectorizer_description = load(p)

with open("/content/tfidf_description.pkl", "rb") as p:
    tfidf_description = load(p)

def search(vectorizer):
  query = str(input('Enter the Description: '))
  processed = re.sub("[^a-zA-Z0-9 ]", "", query.lower())
  query_vec = vectorizer.transform([query])
  similarity = cosine_similarity(query_vec, tfidf_description).flatten()
  indices = np.argpartition(similarity, -50)[-50:]
  results = df_books_processed.iloc[indices]
  return results[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

search(vectorizer_description)

"""### Collaborative Filtering

User-User Similarity
"""

import pandas as pd
import numpy as np
from scipy.sparse import coo_matrix
from sklearn.metrics.pairwise import cosine_similarity

def make_clickable(url):
    return f'<a target="_blank" href="{url}">{url}</a>'

def show_image(url):
    return f'<img src="{url}" width="60" >'

def similar_users(df_books_users):
    # Demander à l'utilisateur d'entrer un ID utilisateur
    user_id = input('Enter user_id: ')

    # Récupérer les livres aimés par l'utilisateur spécifié
    books_liked_by_user = set(df_books_users[df_books_users['user_id'] == user_id]['book_id'])

    # Compter les utilisateurs ayant des livres en commun
    user_counts = df_books_users[df_books_users['book_id'].isin(books_liked_by_user)]['user_id'].value_counts()
    high_count_threshold = np.percentile(user_counts, 99)
    similar_users = user_counts[user_counts >= high_count_threshold].index.tolist()

    # Filtrer les données pour ces utilisateurs similaires
    similar_data = df_books_users[(df_books_users['user_id'].isin(similar_users)) & (df_books_users['book_id'].isin(books_liked_by_user))]

    # Encodage des utilisateurs et des livres
    user_ids_encoded, user_id_mapping = pd.factorize(similar_data['user_id'])
    book_ids_encoded, book_id_mapping = pd.factorize(similar_data['book_id'])

    # Création de la matrice de notes
    ratings_mat_coo = coo_matrix((similar_data["rating"].astype(float), (user_ids_encoded, book_ids_encoded)))
    ratings_mat = ratings_mat_coo.tocsr()

    # Calculer la similarité cosine pour l'utilisateur donné
    target_index = user_id_mapping.get_loc(user_id)
    similarity = cosine_similarity(ratings_mat[target_index, :], ratings_mat).flatten()
    top_similar_indices = np.argsort(similarity)[-1:-51:-1]

    # Afficher les livres hautement notés par l'utilisateur
    top_books = df_books_users[df_books_users['user_id'] == user_id].nlargest(20, 'rating')['book_id']
    book_titles = df_books_users[df_books_users['book_id'].isin(top_books)]['title_without_series'].unique()
    print('Books highly rated by given user: \n')
    for count, title in enumerate(book_titles, 1):
        print(f"{count}. {title}")

    return similar_data[similar_data['user_id'].isin(top_similar_indices)]

# Assume df_books_users is defined
df_similar_users_refined = similar_users(df_books_users)
# You would call recommendation function here, passing df_similar_users_refined as parameter

"""tem-Item Similarity"""

import pandas as pd
import numpy as np
from scipy.sparse import coo_matrix
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder

def make_clickable(url):
    return f'<a target="_blank" href="{url}">{url}</a>'

def show_image(url):
    return f'<img src="{url}" width="60" >'

def similar_item_recommendation(df_books_users, df_books_processed):
    book_id = input('Enter book_id: ')  # Assurez-vous que cette invite s'affiche correctement

    users_who_liked_book = set(df_books_users[df_books_users['book_id'] == book_id]['user_id'])
    if not users_who_liked_book:
        print("No users found for this book.")
        return pd.DataFrame()

    books_id_remaining = df_books_users[df_books_users['user_id'].isin(users_who_liked_book)]

    user_encoder = LabelEncoder()
    book_encoder = LabelEncoder()

    books_id_remaining['user_id_encoded'] = user_encoder.fit_transform(books_id_remaining['user_id'])
    books_id_remaining['book_id_encoded'] = book_encoder.fit_transform(books_id_remaining['book_id'])

    ratings = books_id_remaining['rating'].astype(float)
    ratings_mat_coo = coo_matrix((ratings, (books_id_remaining['book_id_encoded'], books_id_remaining['user_id_encoded'])))
    ratings_mat = ratings_mat_coo.tocsr()

    try:
        book_index = book_encoder.transform([book_id])[0]
    except ValueError:
        print("Book ID not found in the encoded matrix.")
        return pd.DataFrame()

    similarity = cosine_similarity(ratings_mat[book_index, :], ratings_mat).flatten()
    similar_books_indices = np.argsort(similarity)[-2:-52:-1]  # Excluding the book itself
    similar_books_ids = book_encoder.inverse_transform(similar_books_indices)

    score = [(similarity[idx], book) for idx, book in zip(similar_books_indices, similar_books_ids)]
    df_score = pd.DataFrame(score, columns=['score', 'book_id'])

    df_similar_books_to_recommend = df_books_processed[df_books_processed['book_id'].isin(similar_books_ids)]
    df_similar_books_to_recommend = df_similar_books_to_recommend.merge(df_score, on='book_id')

    df_similar_books_to_recommend = df_similar_books_to_recommend.sort_values(by='score', ascending=False).drop_duplicates(subset='book_id')

    original_book_title = df_books_processed[df_books_processed['book_id'] == book_id]['title_without_series'].values
    if original_book_title.size > 0:
        print('Title of book given by customer:', original_book_title[0].strip())
    else:
        print('Book ID not found in df_books_processed.')

    result = df_similar_books_to_recommend[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})
    print(result)  # Explicitly print the result if not automatically displayed

# Usage example (uncomment and modify path accordingly)
# df_books_users = pd.read_csv('path_to_your_books_users.csv')
# df_books_processed = pd.read_csv('path_to_your_books_processed.csv')
# similar_item_recommendation(df_books_users, df_books_processed)

"""SVD(Singular-Valued Decomposition)"""

df_svd = df_books_users_processed[['user_id', 'book_id', 'user_rating']]
reader = Reader(rating_scale=(0, 5))
data = Dataset.load_from_df(df_svd, reader)
trainset, testset = model_selection.train_test_split(data, test_size=0.30)
model = SVD()
model.fit(trainset)
predictions = model.test(testset)
accuracy.rmse(predictions)

with open('/content/model_svd.pkl', 'wb') as p:
    dump(model, p)

def recommendation_svd():
  user_id = str(input('Enter user_id: '))
  book_id = set(df_books_users_processed[df_books_users_processed['user_id']==user_id]['book_id'])
  user_books = df_books_processed[~df_books_processed['book_id'].isin(book_id)]
  user_books['user_id'] = len(user_books)*[user_id]
  user_books.reset_index(drop=True, inplace=True)
  user_books['user_rating'] = 0
  df_svd_predict = Dataset.load_from_df(user_books[['user_id', 'book_id', 'user_rating']], reader)
  NA, test = model_selection.train_test_split(df_svd_predict, test_size=1.0)
  predictions = model.test(test)
  predictions = [prediction.est for prediction in predictions]
  user_books['rating'] = predictions
  top_50_books_for_user_content = user_books.sort_values(by=['rating'], ascending=False)[:50]
  top_50_books_for_user_content.to_csv('top_50_books_for_user_content.csv', index=False)
  book_title_liked_by_user = set(df_books_users_processed[df_books_users_processed['user_id']==user_id].sort_values(by='user_rating', ascending=False)['title_without_series'])
  print('Books highly rated by given user: \n')
  for count, books in enumerate(list(book_title_liked_by_user)[:20]):
    print(count+1,'.  ',books)
  return top_50_books_for_user_content[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page', 'rating']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})


recommendation_svd()

"""Filtering out books that have missing cover pages"""

df_books_with_cover_page = df_books_processed[~df_books_processed['cover_page'].str.contains("https://s.gr-assets.com/assets/nophoto")]

"""Saving images from provided book's cover page url"""

def firstprocess():
   for index, row in (df_books_with_cover_page[:59601].iterrows()):
    url = row['cover_page']
    try:
        response = requests.get(url)
        img = Image.open(BytesIO(response.content))
        if img.mode != 'RGB':
          img = img.convert('RGB')
        img.save('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderone_image/'+ str(row['book_id'])+'.jpeg')
    except Exception:
      pass

# '''def secondprocess():
#   for index, row in (df_books_with_cover_page[59601:119202].iterrows()):
#     url = row['cover_page']
#     try:
#         response = requests.get(url)
#         img = Image.open(BytesIO(response.content))
#         if img.mode != 'RGB':
#           img = img.convert('RGB')
#         img.save('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldertwo_image/'+ str(row['book_id'])+'.jpeg')
#     except Exception:
#       pass

# def thirdprocess():
#   for index, row in (df_books_with_cover_page[119202:178803].iterrows()):
#     url = row['cover_page']
#     try:
#         response = requests.get(url)
#         img = Image.open(BytesIO(response.content))
#         if img.mode != 'RGB':
#           img = img.convert('RGB')
#         img.save('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderthree_image/'+ str(row['book_id'])+'.jpeg')
#     except Exception:
#       pass

# def fourthprocess():
#   for index, row in (df_books_with_cover_page[178803:238404].iterrows()):
#     url = row['cover_page']
#     try:
#         response = requests.get(url)
#         img = Image.open(BytesIO(response.content))
#         if img.mode != 'RGB':
#           img = img.convert('RGB')
#         img.save('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfour_image/'+ str(row['book_id'])+'.jpeg')
#     except Exception:
#       pass

# def fifthprocess():
#   for index, row in (df_books_with_cover_page[238404:298005].iterrows()):
#     url = row['cover_page']
#     try:
#         response = requests.get(url)
#         img = Image.open(BytesIO(response.content))
#         if img.mode != 'RGB':
#           img = img.convert('RGB')
#         img.save('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfive_image/'+ str(row['book_id'])+'.jpeg')
#     except Exception:
#       pass

# def sixthprocess():
#   for index, row in (df_books_with_cover_page[298005:357606].iterrows()):
#     url = row['cover_page']
#     try:
#         response = requests.get(url)
#         img = Image.open(BytesIO(response.content))
#         if img.mode != 'RGB':
#           img = img.convert('RGB')
#         img.save('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldersix_image/'+ str(row['book_id'])+'.jpeg')
#     except Exception:
#       pass

# def seventhprocess():
#   for index, row in (df_books_with_cover_page[357606:417207].iterrows()):
#     url = row['cover_page']
#     try:
#         response = requests.get(url)
#         img = Image.open(BytesIO(response.content))
#         if img.mode != 'RGB':
#           img = img.convert('RGB')
#         img.save('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderseven_image/'+ str(row['book_id'])+'.jpeg')
#     except Exception:
#       pass

# def eigthprocess():
#   for index, row in (df_books_with_cover_page[417207:476815].iterrows()):
#     url = row['cover_page']
#     try:
#         response = requests.get(url)
#         img = Image.open(BytesIO(response.content))
#         if img.mode != 'RGB':
#           img = img.convert('RGB')
#         img.save('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldereight_image/'+ str(row['book_id'])+'.jpeg')
#     except Exception:
#       pass'''

"""Multiprocessing for parallelizing jobs"""

from multiprocessing import Process

def main():
    p1=Process(target=firstprocess)
    # p2=Process(target=secondprocess)
    # p3=Process(target=thirdprocess)
    # p4=Process(target=fourthprocess)
    # p5=Process(target=fifthprocess)
    # p6=Process(target=sixthprocess)
    # p7=Process(target=seventhprocess)
    # p8=Process(target=eigthprocess)
   # p1.start() is used to start the thread execution
    p1.start()
    # p2.start()
    # p3.start()
    # p4.start()
    # p5.start()
    # p6.start()
    # p7.start()
    # p8.start()
    #After completion all the threads are joined
    p1.join()
    # p2.join()
    # p3.join()
    # p4.join()
    # p5.join()
    # p6.join()
    # p7.join()
    # p8.join()

if __name__=="__main__":
    main()

"""**Transfer Learning(VGG16): Extracting feature vectors from saved cover page of books**"""

model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(95, 140, 3))
model_new = Model(model_vgg16.input, model_vgg16.layers[-1].output)

def firstprocess():
  mapper = dict()
  folderone_image = os.listdir('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderone_image')
  for image_path in tqdm(folderone_image):
    image = load_img('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderone_image/'+image_path, target_size=(95, 140))
    image = img_to_array(image)
    image = preprocess_input(image)
    bottleneck_features_train = np.array(model_new(np.expand_dims(image, axis=0))).flatten()
    mapper[int(image_path.split('.')[0])] = bottleneck_features_train
  with open('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderone_image.pkl', 'wb') as p:
      dump(mapper, p)

# def secondprocess():
#   mapper = dict()
#   foldertwo_image = os.listdir('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldertwo_image')
#   for image_path in tqdm(foldertwo_image):
#     image = load_img('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldertwo_image/'+image_path, target_size=(95, 140))
#     image = img_to_array(image)
#     image = preprocess_input(image)
#     bottleneck_features_train = np.array(model_new(np.expand_dims(image, axis=0))).flatten()
#     mapper[image_path.split('.')[0]] = bottleneck_features_train

#   with open('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldertwo_image.pkl', 'wb') as p:
#       dump(mapper, p)

# def thirdprocess():
#   mapper = dict()
#   folderthree_image = os.listdir('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderthree_image')
#   for image_path in tqdm(folderthree_image):
#     image = load_img('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderthree_image/'+image_path, target_size=(95, 140))
#     image = img_to_array(image)
#     image = preprocess_input(image)
#     bottleneck_features_train = np.array(model_new(np.expand_dims(image, axis=0))).flatten()
#     mapper[image_path.split('.')[0]] = bottleneck_features_train

#   with open('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderthree_image.pkl', 'wb') as p:
#       dump(mapper, p)

# def fourthprocess():
#   mapper = dict()
#   folderfour_image = os.listdir('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfour_image')
#   for image_path in tqdm(folderfour_image):
#     image = load_img('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfour_image/'+image_path, target_size=(95, 140))
#     image = img_to_array(image)
#     image = preprocess_input(image)
#     bottleneck_features_train = np.array(model_new(np.expand_dims(image, axis=0))).flatten()
#     mapper[image_path.split('.')[0]] = bottleneck_features_train

#   with open('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfour_image.pkl', 'wb') as p:
#       dump(mapper, p)

# def fifthprocess():
#   mapper = dict()
#   folderfive_image = os.listdir('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfive_image')
#   for image_path in tqdm(folderfive_image):
#     image = load_img('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfive_image/'+image_path, target_size=(95, 140))
#     image = img_to_array(image)
#     image = preprocess_input(image)
#     bottleneck_features_train = np.array(model_new(np.expand_dims(image, axis=0))).flatten()
#     mapper[image_path.split('.')[0]] = bottleneck_features_train

#   with open('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfive_image.pkl', 'wb') as p:
#       dump(mapper, p)


# def sixthprocess():
#   mapper = dict()
#   foldersix_image = os.listdir('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldersix_image')
#   for image_path in tqdm(foldersix_image):
#     image = load_img('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldersix_image/'+image_path, target_size=(95, 140))
#     image = img_to_array(image)
#     image = preprocess_input(image)
#     bottleneck_features_train = np.array(model_new(np.expand_dims(image, axis=0))).flatten()
#     mapper[image_path.split('.')[0]] = bottleneck_features_train

#   with open('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldersix_image.pkl', 'wb') as p:
#       dump(mapper, p)


# def seventhprocess():
#   mapper = dict()
#   folderseven_image = os.listdir('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderseven_image')
#   for image_path in tqdm(folderseven_image):
#     image = load_img('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderseven_image/'+image_path, target_size=(95, 140))
#     image = img_to_array(image)
#     image = preprocess_input(image)
#     bottleneck_features_train = np.array(model_new(np.expand_dims(image, axis=0))).flatten()
#     mapper[image_path.split('.')[0]] = bottleneck_features_train

#   with open('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderseven_image.pkl', 'wb') as p:
#       dump(mapper, p)


# def eigthprocess():
#   mapper = dict()
#   foldereight_image = os.listdir('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldereight_image')
#   for image_path in tqdm(foldereight_image):
#     image = load_img('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldereight_image/'+image_path, target_size=(95, 140))
#     image = img_to_array(image)
#     image = preprocess_input(image)
#     bottleneck_features_train = np.array(model_new(np.expand_dims(image, axis=0))).flatten()
#     mapper[image_path.split('.')[0]] = bottleneck_features_train

#   with open('/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/ ', 'wb') as p:
#       dump(mapper, p)

firstprocess()
# secondprocess()
# thirdprocess()
# fourthprocess()
# fifthprocess()
# sixthprocess()
# seventhprocess()
# eigthprocess()

"""Observation:

*  Each function created above took nearly 6 min on GPU(6*8=48 minutes)

* Loading saved features*
"""

mapper1 = load(open("/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderone_image.pkl", "rb"))
mapper2 = load(open("/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldertwo_image.pkl", "rb"))
mapper3 = load(open("/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderthree_image.pkl", "rb"))
mapper4 = load(open("/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfour_image.pkl", "rb"))
mapper5 = load(open("/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderfive_image.pkl", "rb"))
mapper6 = load(open("/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldersix_image.pkl", "rb"))
mapper7 = load(open("/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/folderseven_image.pkl", "rb"))
mapper8 = load(open("/content/drive/MyDrive/FatimaEzz/REC_SYS/goodreads/CSV/foldereight_image.pkl", "rb"))

"""**Translating features vector dimensions**

Note: This part can be taken care while extracting features through Transfer Learning
"""

mapper1 = dict(map(lambda x: (x[0], x[1].reshape(1, -1)), mapper1.items()))
mapper2 = dict(map(lambda x: (x[0], x[1].reshape(1, -1)), mapper2.items()))
mapper3 = dict(map(lambda x: (x[0], x[1].reshape(1, -1)), mapper3.items()))
mapper4 = dict(map(lambda x: (x[0], x[1].reshape(1, -1)), mapper4.items()))
mapper5 = dict(map(lambda x: (x[0], x[1].reshape(1, -1)), mapper5.items()))
mapper6 = dict(map(lambda x: (x[0], x[1].reshape(1, -1)), mapper6.items()))
mapper7 = dict(map(lambda x: (x[0], x[1].reshape(1, -1)), mapper7.items()))
mapper8 = dict(map(lambda x: (x[0], x[1].reshape(1, -1)), mapper8.items()))

"""**Merging all dictionaries, creating dataframe**"""

merge = {**mapper1, **mapper2, **mapper3, **mapper4, **mapper5, **mapper6, **mapper7, **mapper8}
df_image = pd.DataFrame(list(merge.items()), columns=['book_id', 'features'])
features = np.squeeze(np.array(list(df_image['features'])), axis=1)
df_image['book_id'] = df_image['book_id'].astype(int)

def get_similar_books_cnn():
  book_id = int(input('Enter the book_id: '))
  index = df_image[df_image['book_id']== book_id].index
  pairwise_dist = pairwise_distances(features, features[index].reshape(1,-1), metric='cosine')
  indices = list(np.argsort(pairwise_dist.flatten())[0:5])
  book_ids = list(df_image.loc[indices]['book_id'])
  score = [(score, book) for score, book in enumerate(book_ids)]
  df_score = pd.DataFrame(score, columns =['score', 'book_id'])
  similar_books= (df_books_processed[df_books_processed['book_id'].isin(book_ids)].merge(df_score, on='book_id')).sort_values(by='score')
  print('Entered book title: ', str(df_books_processed[df_books_processed['book_id']==int(book_id)]['title_without_series'].values[0]).strip())
  return similar_books[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

get_similar_books_cnn()

"""### **Books that got similar review**"""

data = list()
for book_id, df in tqdm(df_users[['book_id', 'review_text']].groupby('book_id')):
  data.append((book_id, list(df['review_text'])))

df_book_id_review = pd.DataFrame(data, columns =['book_id', 'list_review_text'])
df_book_id_review

df_final_review = df_book_id_review[df_book_id_review['book_id'].isin(set(df_books_processed['book_id']))]
df_final_review.reset_index(drop=True, inplace=True)

stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've",\
            "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \
            'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their',\
            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', \
            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \
            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \
            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\
            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\
            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\
            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \
            's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', \
            've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn',\
            "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn',\
            "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", \
            'won', "won't", 'wouldn', "wouldn't"])


def decontracted(phrase):
    # specific
    phrase = re.sub(r"won't", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)

    # general
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    return phrase

# Convertir les book_id en entiers dans df_book_id_review
df_book_id_review['book_id'] = df_book_id_review['book_id'].astype(int)

# Convertir les book_id en entiers dans df_books_processed (si ce n'est pas déjà fait)
df_books_processed['book_id'] = df_books_processed['book_id'].astype(int)

# Vérifiez les IDs après la conversion
print("Book IDs après conversion dans df_book_id_review:")
print(df_book_id_review['book_id'].unique())

print("Book IDs après conversion dans df_books_processed:")
print(df_books_processed['book_id'].unique())

# Filtrer df_book_id_review pour obtenir df_final_review
df_final_review = df_book_id_review[df_book_id_review['book_id'].isin(set(df_books_processed['book_id']))]
df_final_review.reset_index(drop=True, inplace=True)

# Vérifiez la taille de df_final_review après filtrage
print("Nombre de critiques dans df_final_review après filtrage:", len(df_final_review))
print("Book IDs dans df_final_review après filtrage:")
print(df_final_review['book_id'].unique())

# Vérification des données de départ
print("Vérification des données initiales:")
print(df_final_review.head())

# Compter le nombre de critiques par livre pour vérifier si la colonne est bien remplie
print("Nombre de critiques par livre:")
print(df_final_review['list_review_text'].apply(len).describe())

preprocessed_reviews = list()
for sentences in tqdm(df_final_review['list_review_text'].values):
  mod_sentences = list()
  for sentence in sentences:
    try:
      sentence = re.sub(r"http\S+", "", sentence)
      sentence = decontracted(sentence)
      sentence = re.sub('[^A-Za-z]+', ' ', sentence[:500])
      sentence = BeautifulSoup(sentence, 'lxml').get_text()

      sentence = re.sub("\S*\d\S*", "", sentence).strip()
      sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)
      mod_sentences.append(sentence.strip())
    except Exception:
      pass
  preprocessed_reviews.append(mod_sentences)

# Combinaison des critiques prétraitées en une seule chaîne
review = [' '.join(lst) for lst in preprocessed_reviews]

# Ajout des critiques au DataFrame
df_final_review['combined_processed_review'] = review
df_final_review['preprocessed_reviews'] = preprocessed_reviews

# Vérification de l'état du DataFrame
print("État du DataFrame après combinaison des critiques:")
print(df_final_review.head())

# Vérification des critiques combinées pour assurer qu'elles ne sont pas vides
non_empty_reviews = df_final_review['combined_processed_review'].apply(lambda x: x.strip() != '')
print(f"Nombre de critiques non vides : {non_empty_reviews.sum()} / {len(df_final_review)}")

# Vérification des critiques combinées
for i, r in enumerate(df_final_review['combined_processed_review']):
    if not r.strip():
        print(f"Review at index {i} is empty.")
    else:
        print(f"Review {i}: {r[:100]}...")  # Afficher les 100 premiers caractères

# Filtrer les critiques non vides avant d'appliquer TF-IDF
non_empty_reviews_df = df_final_review[df_final_review['combined_processed_review'].apply(lambda x: x.strip() != '')]

# Application du TF-IDF Vectorizer sur les critiques non vides
vectorizer_review = TfidfVectorizer(stop_words=None, min_df=1)
tfidf_review = vectorizer_review.fit_transform(non_empty_reviews_df['combined_processed_review'])

# Vérification du résultat TF-IDF
print(f"TF-IDF Shape: {tfidf_review.shape}")
feature_names = vectorizer_review.get_feature_names_out()
print(f"Premiers noms de caractéristiques : {feature_names[:10]}")

preprocessed_reviews = list()
for sentences in tqdm(df_final_review['list_review_text'].values):
  mod_sentences = list()
  for sentence in sentences:
    try:
      sentence = re.sub(r"http\S+", "", sentence)
      sentence = decontracted(sentence)
      sentence = re.sub('[^A-Za-z]+', ' ', sentence[:500])
      sentence = BeautifulSoup(sentence, 'lxml').get_text()

      sentence = re.sub("\S*\d\S*", "", sentence).strip()
      sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)
      mod_sentences.append(sentence.strip())
    except Exception:
      pass
  preprocessed_reviews.append(mod_sentences)

review = [' '.join(lst) for lst in tqdm(preprocessed_reviews)]

df_final_review['combined_processed_review'] = review
df_final_review['preprocessed_reviews'] = preprocessed_reviews

vectorizer_review = TfidfVectorizer()
tfidf_review = vectorizer_review.fit_transform(df_final_review['combined_processed_review'])

with open('/content/vectorizer_review.pkl', 'wb') as p:
    dump(vectorizer_review, p)

with open('/content/tfidf_review.pkl', 'wb') as p:
    dump(tfidf_review, p)

with open("/content/vectorizer_review.pkl", "rb") as p:
    vectorizer_review = load(p)

with open("/content/tfidf_review.pkl", "rb") as p:
    tfidf_review = load(p)

df_final_review.to_csv('df_final_review.csv.gz', compression='gzip', index=False)

print(df_final_review['book_id'].dtype)  # Vérifiez le type de book_id dans df_final_review

df_final_review['book_id'] = df_final_review['book_id'].astype(str)  # Convertir en chaîne

print(df_final_review['book_id'].unique())  # Affiche les book_id uniques

print(tfidf_review.shape)  # Dimensions de la matrice TF-IDF

print(df_final_review.head())  # Affiche les premières lignes de df_final_review pour vérifier les book_id

df_books_processed['book_id'] = df_books_processed['book_id'].astype(str)

print(df_final_review[df_final_review['book_id'] == '10000'])

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Définition des fonctions de formatage
def make_clickable(val):
    return f'<a href="{val}" target="_blank">{val}</a>'

def show_image(val):
    return f'<img src="{val}" width="60">'

def review_similarity():
    # Entrer le book_id
    book_id = input('Enter the book_id: ').strip()

    # Vérifier si le book_id existe dans df_final_review
    if book_id not in df_final_review['book_id'].astype(str).values:
        print(f"Error: book_id {book_id} not found in df_final_review.")
        print("Available book_ids in df_final_review:")
        print(df_final_review['book_id'].unique())
        return

    # Trouver l'index du livre
    index = df_final_review[df_final_review['book_id'] == book_id].index

    if len(index) == 0:
        print(f"Error: No reviews found for book_id {book_id}.")
        return

    # Calculer la similarité cosinus
    similarity = cosine_similarity(tfidf_review[index], tfidf_review).flatten()

    # Vérifier si la similarité a renvoyé des résultats
    if similarity.size == 0:
        print("Error: Similarity calculation failed. No data found.")
        return

    # Trouver les indices des livres les plus similaires
    indices = np.argpartition(similarity, -50)[-50:]
    book_ids = set(df_final_review.iloc[indices]['book_id'])

    # Créer un score pour chaque livre similaire
    score = [(score, book) for score, book in enumerate(book_ids)]
    df_score = pd.DataFrame(score, columns=['score', 'book_id'])

    # Obtenir les informations des livres similaires
    results = (df_books_processed[df_books_processed['book_id'].astype(str).isin(book_ids)]
               .merge(df_score, on='book_id')
               .sort_values(by='score'))

    # Afficher le titre du livre entré
    title_series = df_books_processed[df_books_processed['book_id'].astype(str) == book_id]['title_without_series']
    if not title_series.empty:
        book_title = title_series.values[0].strip()
        print('Entered book title: ', book_title)
    else:
        print('Error: Title not found for the entered book_id.')
        print("Available book_ids in df_books_processed:")
        print(df_books_processed['book_id'].unique())

    # Retourner les résultats
    return results[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page', 'ratings_count']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

# Appeler la fonction
review_similarity()

"""### *Correlation-Based Recommendation*"""

user = df_books_users_processed['user_id'].value_counts().to_frame().reset_index()
user.columns = ['user_id', 'user_rating_count']
highly_active_users = user[user['user_rating_count']>300]['user_id']
highly_rated_books = df_books_processed[df_books_processed['ratings_count']>3000]['book_id']

import pandas as pd
import numpy as np
# Définition des fonctions de formatage
def make_clickable(val):
    return f'<a href="{val}" target="_blank">{val}</a>'

def show_image(val):
    return f'<img src="{val}" width="60">'
def correlation_recommendation():
    # Filtrer les utilisateurs et les livres hautement actifs et hautement notés
    confined_df_users_books = df_books_users_processed[
        (df_books_users_processed['user_id'].isin(highly_active_users)) &
        (df_books_users_processed['book_id'].isin(highly_rated_books))
    ]
    confined_df_users_books_limited = confined_df_users_books[['user_id', 'book_id', 'rating']]

    # Créer une table pivot
    pivot_books = confined_df_users_books_limited.pivot(index='user_id', columns='book_id').rating

    # Saisir l'identifiant du livre
    book_id = int(input('Enter the book_id: '))

    # Vérifier si le book_id existe dans pivot_books
    if book_id not in pivot_books.columns:
        print(f"Error: book_id {book_id} not found in the dataset.")
        return

    # Calculer la corrélation
    pivot_book_id = pivot_books[book_id]
    scores = pivot_books.corrwith(pivot_book_id).to_frame().reset_index()
    scores.columns = ['book_id', 'pearson_score']
    scores.dropna(inplace=True)

    # Trouver les livres similaires
    similar_books_to_given_book = (
        df_books_processed[df_books_processed['book_id'].isin(scores[scores['pearson_score'] > 0.0]['book_id'])]
        .merge(scores[scores['pearson_score'] > 0.0], on='book_id')
        .sort_values(by='pearson_score', ascending=False)
    )

    # Afficher le titre du livre donné par le client
    title_series = df_books_processed[df_books_processed['book_id'] == book_id]['title_without_series']
    if not title_series.empty:
        book_title = title_series.values[0].strip()
        print('Title of book given by customer:', book_title)
    else:
        print('Error: Title not found for the entered book_id.')

    # Retourner les résultats formatés

    return similar_books_to_given_book[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

# Appeler la fonction
correlation_recommendation()

"""### **Content Based Filtering**

Finding Tf-idf score for mod_title
"""

vectorizer_title = TfidfVectorizer(max_df= 659408, min_df= 100,  stop_words='english')
vectorizer_title.fit(df_books_processed["mod_title"])
tfidf_title = vectorizer_title.transform(df_books_users_processed['mod_title'])
tfidf_title.shape

"""Cleaning review_text data"""

def processing(review):
  sentence = re.sub(r"http\S+", "", review)
  sentence = decontracted(sentence)
  sentence = re.sub('[^A-Za-z]+', ' ', sentence[:500])
  sentence = BeautifulSoup(sentence, 'lxml').get_text()
  sentence = re.sub("\S*\d\S*", "", sentence).strip()
  sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)
  return sentence

df_books_users_processed['review_text'] = df_books_users_processed['review_text'].apply(processing)

"""Finding Tf-idf score for review_text"""

vectorizer_review = TfidfVectorizer(max_df= 659408, min_df= 500, stop_words='english')
vectorizer_review.fit(df_books_users_processed["review_text"])
tfidf_review = vectorizer_review.transform(df_books_users_processed['review_text'])
tfidf_review.shape

"""Saving necessary data"""

with open('/content/tfidf_title.pkl', 'wb') as p:
    dump(tfidf_title, p)
with open('/content/vectorizer_title.pkl', 'wb') as p:
    dump(vectorizer_title, p)

with open('/content/tfidf_review.pkl', 'wb') as p:
    dump(tfidf_review, p)
with open('/content/vectorizer_review.pkl', 'wb') as p:
    dump(vectorizer_review, p)

"""Label Encoding categorical features"""

le1 = preprocessing.LabelEncoder()
le1.fit(df_books_users_processed['book_id'])
df_books_users_processed['book_id_mapped'] = le1.transform(df_books_users_processed['book_id'])

le4 = preprocessing.LabelEncoder()
le4.fit(df_books_users_processed['publisher'])
df_books_users_processed['publisher_mapped'] = le4.transform(df_books_users_processed['publisher'])

le7 = preprocessing.LabelEncoder()
le7.fit(df_books_users_processed['is_ebook'])
df_books_users_processed['is_ebook_mapped'] = le7.transform(df_books_users_processed['is_ebook'])

le9 = preprocessing.LabelEncoder()
le9.fit(df_books_users_processed['user_id'])
df_books_users_processed['user_id_mapped'] = le9.transform(df_books_users_processed['user_id'])

"""Saving necessary data"""

with open('/content/le1.pkl', 'wb') as p:
    dump(le1, p)
with open('/content/le4.pkl', 'wb') as p:
    dump(le4, p)
with open('/content/le7.pkl', 'wb') as p:
    dump(le7, p)
with open('/content/le9.pkl', 'wb') as p:
    dump(le9, p)

df_books_users_processed.to_csv('df_books_users_processed.csv.gz', index=False, compression='gzip')

"""Extracting useful features for model training"""

df_books_users_numeric = df_books_users_processed[['book_id_mapped', 'publisher_mapped', 'is_ebook_mapped', 'user_id_mapped', 'publication_year', 'ratings_count', 'book_average_rating', 'num_pages', 'rating']]

"""Input and Output data for model training"""

x = df_books_users_numeric.drop(columns=['rating'])
user_rating = df_books_users_numeric['rating']

"""Normalizing Data"""

norm = MinMaxScaler()
norm.fit(x)
x_normalized = norm.transform(x)

with open("/content/norm.pkl", "wb") as p:
    dump(norm, p)

"""Stacking all features"""

data_x = hstack((x_normalized, tfidf_title, tfidf_review))

"""Saving the prepared training dataset"""

scipy.sparse.save_npz('/content/data_x.npz', data_x)
with open('/content/user_rating.pkl', 'wb') as p:
    dump(user_rating, p)

"""Loading the dataset"""

data_x = scipy.sparse.load_npz('/content/data_x.npz')
with open("/content/user_rating.pkl", "rb") as p:
    user_rating = load(p)

"""Train-Test split"""

X_train, X_test, y_train, y_test = train_test_split(data_x, user_rating, test_size=0.30, random_state=42)

""" **GridSearch on Logistic regression to find best suited parameters for model**"""

lr = OneVsRestClassifier(LogisticRegression())
param_grid = {'estimator__penalty':['l1','l2'],'estimator__class_weight':[None, 'balanced']}
gs_lr = GridSearchCV(lr, param_grid, cv= 2, scoring='f1_macro', verbose=2, return_train_score=True, n_jobs=-1)
gs_lr.fit(X_train, y_train)

"""Finding best parameters for model"""

best_params = gs_lr.best_params_
print('Best parameters are: ', best_params)

"""Training model with best obtained Parameters"""

clf_lr = OneVsRestClassifier(LogisticRegression())
clf_lr.fit(X_train, y_train)

"""Evaluating scores obtained on trained model"""

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Prédictions sur les données d'entraînement
predicted_train = clf_best.predict(X_train)  # 'clf_best' étant le modèle de régression logistique entraîné
# Prédictions sur les données de test
predicted_test = clf_best.predict(X_test)

# Métriques sur les données d'entraînement
precision_train = precision_score(y_train, predicted_train, average='macro')
recall_train = recall_score(y_train, predicted_train, average='macro')
f1_train = f1_score(y_train, predicted_train, average='macro')
accuracy_train = accuracy_score(y_train, predicted_train)

# Métriques sur les données de test
precision_test = precision_score(y_test, predicted_test, average='macro')
recall_test = recall_score(y_test, predicted_test, average='macro')
f1_test = f1_score(y_test, predicted_test, average='macro')
accuracy_test = accuracy_score(y_test, predicted_test)

# Affichage des résultats
print("Régression Logistique - Classification Metrics")
print(f"Train - Precision: {precision_train:.4f}, Recall: {recall_train:.4f}, F1-Score: {f1_train:.4f}, Accuracy: {accuracy_train:.4f}")
print(f"Test - Precision: {precision_test:.4f}, Recall: {recall_test:.4f}, F1-Score: {f1_test:.4f}, Accuracy: {accuracy_test:.4f}")

"""Confusion Matrix for Precision Score"""

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# Supposons que clf_lr, X_train, y_train, X_test, y_test soient déjà définis

# Créer des sous-graphes
figure, axes = plt.subplots(1, 2, figsize=(25, 10))

# Afficher la matrice de confusion pour les données d'entraînement
train_cm_display = ConfusionMatrixDisplay.from_estimator(
    clf_lr,
    X_train,
    y_train,
    normalize='pred',
    cmap='gist_gray',
    ax=axes[0]
)
train_cm_display.im_.set_clim(0, 1)  # Fixer les limites de couleur pour une échelle de couleur cohérente
axes[0].set_title('Score de Précision sur l’Ensemble d’Entraînement')

# Afficher la matrice de confusion pour les données de test
test_cm_display = ConfusionMatrixDisplay.from_estimator(
    clf_lr,
    X_test,
    y_test,
    normalize='pred',
    cmap='gist_gray',
    ax=axes[1]
)
test_cm_display.im_.set_clim(0, 1)  # Fixer les limites de couleur pour une échelle de couleur cohérente
axes[1].set_title('Score de Précision sur l’Ensemble de Test')

plt.show()

with open('/content/clf_lr.pkl', 'wb') as p:
    dump(clf_lr, p)

""" **GridSearch on DecisionTree to find best suited parameters for model**"""

clf = DecisionTreeClassifier()
parameters = {'max_depth':[15, 17],'min_samples_split':[20]}
clf1 = GridSearchCV(clf, parameters, cv= 2, scoring='f1_macro', verbose=2, return_train_score=True, n_jobs=-1)
clf1.fit(X_train, y_train)

"""Finding best parameters for model"""

best_params = clf1.best_params_
print('Best parameters are: ', best_params)

"""Training model with best obtained Parameters"""

clf_best = DecisionTreeClassifier(max_depth=20, min_samples_split=20)
clf_best.fit(X_train, y_train)

"""Evaluating scores obtained on trained model"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Prédictions sur les données d'entraînement
y_train_pred = clf_best.predict(X_train)

# Prédictions sur les données de test
y_test_pred = clf_best.predict(X_test)

# Calcul des métriques pour l'ensemble d'entraînement
accuracy_train = accuracy_score(y_train, y_train_pred)
precision_train = precision_score(y_train, y_train_pred, average='macro')
recall_train = recall_score(y_train, y_train_pred, average='macro')
f1_train = f1_score(y_train, y_train_pred, average='macro')
confusion_train = confusion_matrix(y_train, y_train_pred)

# Calcul des métriques pour l'ensemble de test
accuracy_test = accuracy_score(y_test, y_test_pred)
precision_test = precision_score(y_test, y_test_pred, average='macro')
recall_test = recall_score(y_test, y_test_pred, average='macro')
f1_test = f1_score(y_test, y_test_pred, average='macro')
confusion_test = confusion_matrix(y_test, y_test_pred)

# Affichage des résultats
print("Decision Tree Classifier:")
print(f"Train - Accuracy: {accuracy_train:.4f}, Precision: {precision_train:.4f}, Recall: {recall_train:.4f}, F1-Score: {f1_train:.4f}")
print(f"Test - Accuracy: {accuracy_test:.4f}, Precision: {precision_test:.4f}, Recall: {recall_test:.4f}, F1-Score: {f1_test:.4f}")

"""Confusion Matrix for Precision Score"""

figure, axes = plt.subplots(1, 2, figsize=(25,10))
plot_confusion_matrix(clf_best, X_train, y_train, values_format='.3g', ax=axes[0], normalize='pred', cmap='gist_gray')
plot_confusion_matrix(clf_best, X_test, y_test, values_format='.3g', ax=axes[1], normalize='pred', cmap='gist_gray')
axes[0].set_title('Train Precision score')
axes[1].set_title('Test Precision score')
plt.show()

""" **GridSearch on RandomForest to find best suited parameters for model**"""

clf = RandomForestClassifier()
parameters = {'max_depth':[15],'min_samples_split':[20, 25]}
clf1 = GridSearchCV(clf, parameters, cv= 2, scoring='f1_macro', verbose=2, return_train_score=True, n_jobs=-1)
clf1.fit(X_train, y_train)

"""Evaluating scores and best parameters for model"""

bestScore = clf1.best_score_
best_params = clf1.best_params_
print('Best parameters are: ', best_params)

"""Training model with best obtained Parameters"""

clf_best_rf = RandomForestClassifier(max_depth=15, min_samples_split=25)
clf_best_rf.fit(X_train, y_train)

"""Finding Score of trained model"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Prédictions sur les données d'entraînement
y_train_pred_rf = clf_best_rf.predict(X_train)

# Prédictions sur les données de test
y_test_pred_rf = clf_best_rf.predict(X_test)

# Calcul des métriques pour l'ensemble d'entraînement
accuracy_train_rf = accuracy_score(y_train, y_train_pred_rf)
precision_train_rf = precision_score(y_train, y_train_pred_rf, average='macro')
recall_train_rf = recall_score(y_train, y_train_pred_rf, average='macro')
f1_train_rf = f1_score(y_train, y_train_pred_rf, average='macro')
confusion_train_rf = confusion_matrix(y_train, y_train_pred_rf)

# Calcul des métriques pour l'ensemble de test
accuracy_test_rf = accuracy_score(y_test, y_test_pred_rf)
precision_test_rf = precision_score(y_test, y_test_pred_rf, average='macro')
recall_test_rf = recall_score(y_test, y_test_pred_rf, average='macro')
f1_test_rf = f1_score(y_test, y_test_pred_rf, average='macro')
confusion_test_rf = confusion_matrix(y_test, y_test_pred_rf)

# Affichage des résultats
print("Random Forest Classifier:")
print(f"Train - Accuracy: {accuracy_train_rf:.4f}, Precision: {precision_train_rf:.4f}, Recall: {recall_train_rf:.4f}, F1-Score: {f1_train_rf:.4f}")
print(f"Test - Accuracy: {accuracy_test_rf:.4f}, Precision: {precision_test_rf:.4f}, Recall: {recall_test_rf:.4f}, F1-Score: {f1_test_rf:.4f}")

"""Confusion Matrix for Precision Score"""

figure, axes = plt.subplots(1, 2, figsize=(25,10))
plot_confusion_matrix(clf_best_rf, X_train, y_train, values_format='.3g', ax=axes[0], normalize='pred', cmap='gist_gray')
plot_confusion_matrix(clf_best_rf, X_test, y_test, values_format='.3g', ax=axes[1], normalize='pred', cmap='gist_gray')
axes[0].set_title('Train Precision score')
axes[1].set_title('Test Precision score')
plt.show()

"""**Regression Algorithm**"""

regressor = DecisionTreeRegressor()
parameters = {'max_depth':[15, 20, 30],'min_samples_split':[20, 23, 25]}
regressor1 = GridSearchCV(regressor, parameters, cv= 2, scoring='neg_root_mean_squared_error', verbose=2, return_train_score=True, n_jobs=-1)
regressor1.fit(X_train, y_train)

best_params = regressor1.best_params_
print('Best parameters are: ', best_params)

regressor_best = DecisionTreeRegressor(max_depth=15, min_samples_split=25)
regressor_best.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Prédictions sur les données d'entraînement
y_train_pred = regressor_best.predict(X_train)

# Prédictions sur les données de test
y_test_pred = regressor_best.predict(X_test)

# Calcul des métriques pour l'ensemble d'entraînement
mse_train = mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(mse_train)
mae_train = mean_absolute_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)

# Calcul des métriques pour l'ensemble de test
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = np.sqrt(mse_test)
mae_test = mean_absolute_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)

# Affichage des résultats
print("Decision Tree Regressor:")
print(f"Train - MSE: {mse_train:.4f}, RMSE: {rmse_train:.4f}, MAE: {mae_train:.4f}, R²: {r2_train:.4f}")
print(f"Test - MSE: {mse_test:.4f}, RMSE: {rmse_test:.4f}, MAE: {mae_test:.4f}, R²: {r2_test:.4f}")

"""Observation:

*   Of all the different algorithms we used for training purpose, Logistic Regression classifier gives best performance
*   We will use Logistic Regression trained model for predicting the ratings

Recommendation to users
"""

def content_recommendation():
    user_id = str(input('Entrez l\'user_id : '))

    # Vérifiez que l'user_id existe
    if user_id not in df_books_users_processed['user_id'].values:
        print(f"Erreur : l'user_id {user_id} n'existe pas.")
        return

    # Obtenez les livres que l'utilisateur a déjà lus
    book_id = set(df_books_users_processed[df_books_users_processed['user_id'] == user_id]['book_id'])
    print(f"Livres lus par l'utilisateur {user_id} : {book_id}")

    # Filtrer les livres que l'utilisateur n'a pas encore lus
    user_books = df_books_processed[~df_books_processed['book_id'].isin(list(book_id))].merge(df_final_review, on='book_id')

    # Vérifiez que le DataFrame user_books n'est pas vide
    if user_books.empty:
        print("Aucun livre trouvé pour cet utilisateur après filtrage.")
        return

    user_books['user_id'] = len(user_books) * [user_id]
    user_books.reset_index(drop=True, inplace=True)

    # Filtrer les livres selon les classes disponibles
    print(f"Classes disponibles pour les livres : {le1.classes_[:10]} ...")  # Affiche les 10 premières classes

    user_books = user_books[user_books['book_id'].isin(le1.classes_)]
    print(f"Livres après filtrage par classes : {user_books['book_id'].unique()}")

    # Vérifiez que user_books n'est pas vide après le filtrage par classes
    if user_books.empty:
        print("Aucun livre trouvé après filtrage par classes.")
        return

    # Mapping des attributs catégoriels
    user_books['book_id_mapped'] = le1.transform(user_books['book_id'])
    user_books['publisher_mapped'] = le4.transform(user_books['publisher'])
    user_books['is_ebook_mapped'] = le7.transform(user_books['is_ebook'])
    user_books['user_id_mapped'] = le9.transform(user_books['user_id'])

    # Vérifiez que les colonnes nécessaires existent
    if 'mod_title' not in user_books or 'combined_processed_review' not in user_books:
        print("Erreur : Les colonnes 'mod_title' ou 'combined_processed_review' sont manquantes.")
        return

    # Transformer les titres et critiques avec TF-IDF
    tfidf_title = vectorizer_title.transform(user_books['mod_title'])
    tfidf_review = vectorizer_review.transform(user_books['combined_processed_review'])

    # Vérifier si les transformations TF-IDF produisent des résultats
    if tfidf_title.shape[0] == 0 or tfidf_review.shape[0] == 0:
        print("Erreur : Transformation TF-IDF échouée.")
        return

    # Préparer les données pour le modèle
    user_book_numeric = user_books[['book_id_mapped', 'publisher_mapped', 'is_ebook_mapped', 'user_id_mapped', 'publication_year', 'ratings_count', 'book_average_rating', 'num_pages']]

    # Normaliser les données
    data_scaled = norm.transform(user_book_numeric)
    data_scaled = hstack((data_scaled, tfidf_title, tfidf_review), dtype=np.float32)

    # Vérifiez que les données mises à l'échelle ne sont pas vides
    if data_scaled.shape[0] == 0:
        print("Erreur : Les données mises à l'échelle sont vides.")
        return

    # Prédire les évaluations
    prediction = clf_lr.predict(data_scaled.tocsr())
    user_books['rating'] = prediction

    # Trier et sélectionner les livres les mieux notés pour l'utilisateur
    top_50_books_for_user_content = user_books.sort_values(by=['rating'], ascending=False)[:50]

    # Obtenez les titres des livres appréciés par l'utilisateur
    book_title_liked_by_user = set(df_books_users_processed[df_books_users_processed['book_id'].isin(book_id)].sort_values(by='user_rating', ascending=False)['title_without_series'])

    print('Livres hautement notés par l\'utilisateur donné : \n')
    for count, books in enumerate(list(book_title_liked_by_user)[:20]):
        print(count + 1, '.  ', books)

    return top_50_books_for_user_content[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

# Appeler la fonction
content_recommendation()

# Imprimez le nombre total de classes
print(f"Nombre total de classes : {len(le1.classes_)}")

# Imprimez quelques exemples de classes
print("Exemples de book_id dans le1.classes_: ", le1.classes_[:10])

# Vérifiez le type de données des book_id dans df_books_processed
print("Type de book_id dans df_books_processed : ", df_books_processed['book_id'].dtype)

# Vérifiez le type de données des book_id dans df_books_users_processed
print("Type de book_id dans df_books_users_processed : ", df_books_users_processed['book_id'].dtype)

# Vérifiez le type de données des classes dans le1
print("Type de classes dans le1 : ", type(le1.classes_[0]))

# Convertir les book_id dans df_books_processed en int64
df_books_processed['book_id'] = df_books_processed['book_id'].astype(np.int64)

# Vérification après conversion
print("Type de book_id dans df_books_processed après conversion : ", df_books_processed['book_id'].dtype)

# Vérifiez que tous les book_id de df_books_processed sont dans le1.classes_
books_in_le1 = set(df_books_processed['book_id']).issubset(set(le1.classes_))
print(f"Tous les book_id dans df_books_processed sont présents dans le1.classes_ : {books_in_le1}")

# Vérifiez également pour df_final_review
books_in_le1_review = set(df_final_review['book_id']).issubset(set(le1.classes_))
print(f"Tous les book_id dans df_final_review sont présents dans le1.classes_ : {books_in_le1_review}")

# Affichez quelques exemples de book_id qui ne sont pas dans le1.classes_ si présent
missing_books = [book for book in df_books_processed['book_id'] if book not in le1.classes_]
if missing_books:
    print("Exemples de book_id manquants dans le1.classes_ : ", missing_books[:10])
else:
    print("Tous les book_id sont présents dans le1.classes_.")

# Identifier les book_id qui ne sont pas dans le1.classes_
missing_ids = set(df_final_review['book_id']) - set(le1.classes_)

# Afficher quelques exemples
if missing_ids:
    print("Exemples de book_id manquants dans le1.classes_ : ", list(missing_ids)[:10])
else:
    print("Tous les book_id dans df_final_review sont présents dans le1.classes_.")

# Convertir book_id en int64 dans df_final_review
df_final_review['book_id'] = pd.to_numeric(df_final_review['book_id'], errors='coerce')

# Identifier les book_id manquants après conversion
missing_ids_after_conversion = set(df_final_review['book_id']) - set(le1.classes_)
if missing_ids_after_conversion:
    print("Exemples de book_id manquants après conversion : ", list(missing_ids_after_conversion)[:10])
else:
    print("Tous les book_id dans df_final_review sont maintenant présents dans le1.classes_.")

def content_recommendation():
    user_id = str(input('Entrez l\'user_id : '))
    book_id = set(df_books_users_processed[df_books_users_processed['user_id'] == int(user_id)]['book_id'])
    print(f"Livres lus par l'utilisateur {user_id} : {book_id}")

    # Filtrage des livres que l'utilisateur n'a pas lus
    user_books = df_books_processed[~df_books_processed['book_id'].isin(list(book_id))].merge(df_final_review, on='book_id')

    # Vérification des livres après filtrage
    print("Livres après filtrage : ", user_books['book_id'].unique())

    # Convertir book_id au bon type
    user_books['book_id'] = pd.to_numeric(user_books['book_id'], errors='coerce')

    # Vérifier la présence dans les classes
    user_books = user_books[user_books['book_id'].isin(le1.classes_)]
    print("Livres après filtrage par classes : ", user_books['book_id'].unique())

    # Vérifier si des livres sont trouvés après filtrage par classes
    if user_books.empty:
        print("Aucun livre trouvé après filtrage par classes.")
        return

    user_books['user_id'] = len(user_books) * [user_id]
    user_books.reset_index(drop=True, inplace=True)

    user_books['book_id_mapped'] = le1.transform(user_books['book_id'])
    user_books['publisher_mapped'] = le4.transform(user_books['publisher'])
    user_books['is_ebook_mapped'] = le7.transform(user_books['is_ebook'])
    user_books['user_id_mapped'] = le9.transform(user_books['user_id'])

    # Transformation TF-IDF
    tfidf_title = vectorizer_title.transform(user_books['mod_title'])
    tfidf_review = vectorizer_review.transform(user_books['combined_processed_review'])

    # Préparation des données pour la prédiction
    user_book_numeric = user_books[['book_id_mapped', 'publisher_mapped', 'is_ebook_mapped', 'user_id_mapped', 'publication_year', 'ratings_count', 'book_average_rating', 'num_pages']]
    data_scaled = norm.transform(user_book_numeric)
    data_scaled = hstack((data_scaled, tfidf_title, tfidf_review), dtype=np.float32)

    # Prédiction
    prediction = clf_lr.predict(data_scaled.tocsr())
    user_books['rating'] = prediction

    top_50_books_for_user_content = user_books.sort_values(by=['rating'], ascending=False)[:50]

    # Affichage des livres préférés de l'utilisateur
    book_title_liked_by_user = set(df_books_users_processed[df_books_users_processed['book_id'].isin(book_id)].sort_values(by='user_rating', ascending=False)['title_without_series'])
    print('Livres bien notés par l\'utilisateur donné : \n')
    for count, books in enumerate(list(book_title_liked_by_user)[:20]):
        print(count + 1, '.  ', books)

    return top_50_books_for_user_content[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

# Appeler la fonction
content_recommendation()

def content_recommendation():
  user_id = str(input('Enter user_id: '))
  book_id = set(df_books_users_processed[df_books_users_processed['user_id'] == user_id]['book_id'])
  user_books = df_books_processed[(~df_books_processed['book_id'].isin(list(book_id)))].merge(df_final_review, on='book_id')
  user_books['user_id'] = len(user_books)*[user_id]
  user_books.reset_index(drop=True, inplace=True)
  user_books = user_books[user_books['book_id'].isin(le1.classes_)]
  user_books['book_id_mapped'] = le1.transform(user_books['book_id'])
  user_books['publisher_mapped'] = le4.transform(user_books['publisher'])
  user_books['is_ebook_mapped'] = le7.transform(user_books['is_ebook'])
  user_books['user_id_mapped'] = le9.transform(user_books['user_id'])
  tfidf_title = vectorizer_title.transform(user_books['mod_title'])
  tfidf_review = vectorizer_review.transform(user_books['combined_processed_review'])
  user_book_numeric = user_books[['book_id_mapped', 'publisher_mapped', 'is_ebook_mapped', 'user_id_mapped', 'publication_year', 'ratings_count', 'book_average_rating', 'num_pages']]
  data_scaled = norm.transform(user_book_numeric)
  data_scaled = hstack((data_scaled, tfidf_title, tfidf_review), dtype = np.float32)

  prediction = clf_lr.predict(data_scaled.tocsr())
  user_books['rating'] = prediction
  top_50_books_for_user_content = user_books.sort_values(by=['rating'], ascending=False)[:50]
  book_title_liked_by_user = set(df_books_users_processed[df_books_users_processed['book_id'].isin(book_id)].sort_values(by='rating', ascending=False)['title_without_series'])
  print('Books highly rated by given user: \n')
  for count, books in enumerate(list(book_title_liked_by_user)[:20]):
    print(count+1,'.  ',books)
  return top_50_books_for_user_content[['book_id', 'title_without_series', 'book_average_rating', 'book_url', 'cover_page']].head(5).style.format({'book_url': make_clickable, 'cover_page': show_image})

content_recommendation()

""" ## **Books vedette**"""

# Afficher les premières valeurs de chaque colonne pour vérifier leur format
print(df_books_users['date_added'].head())
print(df_books_users['read_at'].head())

# Vérifier la gamme des dates dans df_books_users
print(df_books_users[['read_at']].describe())

# Vérifier les valeurs manquantes
print(df_books_users['read_at'].isnull().sum())

# Vérifier le type de données
print(df_books_users['read_at'].dtype)

# Vérifiez les types de données
print(df_books_users['read_at'].dtype)

# Affichez les premières lignes pour vérifier les données
print(df_books_users['read_at'].head())

import pandas as pd

# Assurez-vous que df_books_users est votre DataFrame avec les bonnes données

def make_clickable(val):
    return f'<a href="{val}" target="_blank">{val}</a>'

def show_image(val):
    return f'<img src="{val}" width="60">'

def books_vedette():
    # Demander la date à l'utilisateur
    user_input = input("Entrez la date au format 'YYYY-MM-DD': ")

    # Vérifier le format de la date
    try:
        start_date = pd.to_datetime(user_input, format='%Y-%m-%d')
    except ValueError:
        print("Format de date invalide. Veuillez entrer la date au format 'YYYY-MM-DD'.")
        return

    # Assurer que la colonne 'read_at' est en datetime et naïve (sans fuseau horaire)
    df_books_users['read_at'] = pd.to_datetime(df_books_users['read_at'], errors='coerce').dt.tz_localize(None)

    # Assurer que 'start_date' est naïve
    start_date = start_date.tz_localize(None)

    # Filtrer les livres lus après la date spécifiée
    books_after_date = df_books_users[df_books_users['read_at'] >= start_date]

    # Assurer que 'rating' est bien numérique
    df_books_users['rating'] = pd.to_numeric(df_books_users['rating'], errors='coerce')

    # Calculer le nombre de votes pour chaque livre
    top_books = books_after_date.groupby('book_id').agg({
        'rating': 'mean',   # Moyenne des évaluations
        'read_at': 'count'  # Nombre de fois que le livre a été lu
    }).reset_index()

    # Renommer les colonnes pour plus de clarté
    top_books.rename(columns={'rating': 'average_rating', 'read_at': 'n_reads'}, inplace=True)

    # Filtrer les livres ayant été lus au moins 3 fois pour plus de pertinence
    top_books = top_books[top_books['n_reads'] >= 3]

    # Joindre avec les informations supplémentaires des livres
    top_books = top_books.merge(df_books_users[['book_id', 'title_without_series', 'book_url', 'cover_page']].drop_duplicates(), on='book_id')

    # Trier les livres par moyenne d'évaluation et nombre de lectures
    top_books = top_books.sort_values(by=['average_rating', 'n_reads'], ascending=False)

    # Afficher les résultats
    if top_books.empty:
        print("Aucun livre vedette trouvé.")
    else:
        print("Livres vedettes depuis la date spécifiée :")
        display_columns = ['book_id', 'average_rating', 'n_reads']
        print(top_books[display_columns].head(50).to_string(index=False))

# Appeler la fonction
books_vedette()